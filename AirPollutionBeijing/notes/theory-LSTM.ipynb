{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b34f318f-acc6-40f4-852f-574662b54806",
   "metadata": {},
   "source": [
    "# Multivariate Time Series Forecasting\n",
    "\n",
    "For this analysis, we will employ a special type of Neural Netwowrk called ___Long Short Term Memory___ (LSTM), which will be applicable for evaluating ___sequential, time series data___. I want to quickly go over LSTM for my benefit in understanding for this analysis. I use [this site](https://cnvrg.io/pytorch-lstm/) as a helpful source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d926c0-a873-4daa-9e0d-a35d998e6458",
   "metadata": {},
   "source": [
    "---\n",
    "### Purpose\n",
    "\n",
    "Why wouldn't a traditional NN work for this analysis? For the dataset we have, the order of the data, i.e. the Sequence, matters. The traditional NN is restricted due to this, importantly,\n",
    "\n",
    "* They have a fixed input length\n",
    "* They can not remember the sequence of the data, i.e order is not important\n",
    "* Can not share parameters across the sequence\n",
    "\n",
    "For a sequential task, it requires the following:\n",
    "\n",
    "* The model should be able to handle variable-length sequences\n",
    "* Can track Long term dependencies\n",
    "* Maintain information about the order\n",
    "* Share parameters across the sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### Recurrent Neural Network\n",
    "\n",
    "A ___Recurrent Neural Network___ (RNN) is structured like a typical NN (unidirectional), but has loops inside it to persist the information over timestamp $t$. Hence, \"recurrent\". In other words, we have a ___recurrence relation___ applied at every timestamp to process a sequence:\n",
    "\n",
    "\\begin{align}\n",
    "    h_{t} = f_{w}(h_{t - 1}, x_{t})\\, ,\n",
    "\\end{align}\n",
    "\n",
    "where $h_t$ is the current cell state, $f_w$ is a function that is parameterized by weights, $h_{t-1}$ is the previous or last state, and $x_t$ is the input vector at timestamp $t$. _Note that you are using the same function and set of parameters at every timestamp_. For a more complex representation: given the input vector, $x_{t}$, RNN applies a function to update its hidden state which is a standard NN operation:\n",
    "\n",
    "\\begin{align}\n",
    "    h_{t} = \\tanh\\left( W^{T}_{hh}h_{t-1} + W^{T}_{xh}x_{t}\\right)\\, .\n",
    "\\end{align}\n",
    "\n",
    "Here we have 2 separate weight matrices ($W$) then apply the non-linearity ($\\tanh$) to the sum of input $x_t$ and previous state $h_{t-1}$ after multiplication to these 2 weight matrices. Finally, we will have the output vector $\\hat{y}_{t}$ at the timestamp $t$:\n",
    "\n",
    "\\begin{align}\n",
    "    \\hat{y}_{t} = W^{T}_{hy}h_{t} \\, ,\n",
    "\\end{align}\n",
    "\n",
    "which is a modified, transformed version of this internal state, that results simply by multiplication by another weight matrix. This is simply how RNN can update its hidden state and calculate the output.\n",
    "\n",
    "A single cell would look like\n",
    "\n",
    "<img src=\"figures/single-RNN-cell.jpg\" alt=\"fishy\" class=\"bg-primary mb-1\" width=\"200px\">\n",
    "\n",
    "---\n",
    "\n",
    "Consider the __unfolding process__:\n",
    "\n",
    "<img src=\"figures/Unfolding-RNNs.jpg\" alt=\"fishy\" class=\"bg-primary mb-1\" width=\"800px\">\n",
    "\n",
    "* We can see that we are adding the input at every time stamp, and generating the output $\\hat{y}$ at every timestamp. We are going to use the same weight matrices at every timestamp.\n",
    "* $W_{hh}$ is the weight matrix by which you update the previous state, as shown in the equation above, and as visible in the figure. \n",
    "* $W_{xh}$ is the weight matrix that is applied at every timestamp to the input value. Why is the weight matrix that is applied to the output Å·\n",
    "* From these outputs $\\hat{y}_0$, $\\hat{y}_1$, $\\hat{y}_2$, $\\dots$, $\\hat{y}_t$, you can calculate the Loss, $L_{1}$, $L_{2}$, $\\dots$, $L_{t}$, at each timestamp $t$.\n",
    "\n",
    "---\n",
    "\n",
    "### Backpropagation through time in RNN\n",
    "\n",
    "As a __summary__:\n",
    "\n",
    "1) RNN updates the hidden state via input and previous state\n",
    "2) Compute the output matrix via a simple neural network operation that is $W \\times h$\n",
    "3) Return the output and update the hidden state \n",
    "4) You can combine, and take the sum of all these losses to calculate a total loss $L$, through which you can propagate backwards to complete the ___backpropagation___.\n",
    "\n",
    "Backpropagation in RNNs work similarly to backpropagation in Simple Neural Networks, which has the following main steps.\n",
    "\n",
    "1) Feed Forward Pass\n",
    "2) Take the derivative of the loss with each parameter\n",
    "3) Shift parameters to update the weights and minimize the Loss.\n",
    "\n",
    "<img src=\"figures/Backpropagation-in-RNNs.jpg\" alt=\"fishy\" class=\"bg-primary mb-1\" width=\"800px\">\n",
    "\n",
    "The figure above simply \n",
    "\n",
    "* Completes a _feedforward pass_, i.e., computes the output directly from the input, in one pass, (blue to pink via black arrows). \n",
    "* Calculates the loss at each output (khaki Ls).\n",
    "* Takes the derivative of each output.\n",
    "* Propagates backward to update the weights (follow red arrows).\n",
    "\n",
    "Computing the gradients require a lot of factors of Whh plus repeated gradient computations, which makes it a bit problematic, such as exploding gradients (repeated gradient computations, such as weight matrix, or gradient themselves, are greater than 1) or vanishing gradients (like exploding gradients but too small). This can usually be solved using an activation function (Rectified Linear Unit function instead of $\\tanh$), weight initilization, or Changing Network Architecture. We will focus on the latter here, where we modify the architecture of RNNs and use the more complex recurrent unit with Gates such as LSTMs or GRUs (Gated Recurrent Units).\n",
    "\n",
    "---\n",
    "\n",
    "### Long Short Term Memory (LSTMs)\n",
    "\n",
    "LSTMs are a special type of Neural Networks that perform similarly to RNNs, but run better than RNNs, and further solve some of the important shortcomings of RNNs for long term dependencies, and vanishing gradients. ___LSTMs are best suited for long term dependencies___.\n",
    "\n",
    "* LSTMs introduce self-looping to produce paths where gradients can flow for a long duration (meaning gradients will not vanish). \n",
    "* The main contribution of initial long-short-term memory (Hochireiter and Schmidhuber, 1997). \n",
    "* Later on, a crucial addition has been made to make the weight on this self-loop conditioned on the context, rather than fixed. \n",
    "* This can help in changing the time scale of integration. \n",
    "\n",
    "This means that even when LSTM has fixed parameters, the time scale of integration can change based on the input sequence because the time constants are outputs by the model itself.\n",
    "\n",
    "#### LSTMs work in four steps:\n",
    "\n",
    "The key building block behind LSTM is a structure known as __gates__. Information is __added__ or __removed__ through these gates. Gates can optionally let information through, for example via a __sigmoid layer__, and pointwise multiplication.\n",
    "\n",
    "\n",
    "1) ___Forget the irreverent history___:\n",
    "    * Done via the ___forget gate___, its main purpose is to _decide which information the LSTM should keep or carry, and which information it should throw away_. \n",
    "    * This is the function of the prior internal state $h_{t-1}$  and the new input $x_t$.\n",
    "    * This happens because not all the information in a sequence or a sentence needs to be important.\n",
    "    * This forget gate is denoted by $f_{i}^{(t)}$ (for time step $t$ and cell $i$), which sets this weight value between 0 and 1 which decides how much information to send:\n",
    "    \\begin{align}\n",
    "    f_{i}^{(t)} = \\sigma \\left( b_{i}^{f} + \\sum_{j} U_{i,j} x_{j}^{(t)}  + \\sum_{j} W_{i,j}^{f}h_{j}^{(t-1)} \\right)\\, ,\n",
    "    \\end{align} \n",
    "    * Where $x^{(t)}$ is the current input vector, $h^{(t)}$ is the current hidden state, containing the outputs of all the LSTM cells, and $b^f$, $U^f$, $W^f$ are respectively biases, input weights, and recurrent weights for the forget gates.\n",
    "\n",
    "\n",
    "\n",
    "2) ___Perform the computations & store the relevant new information___\n",
    "    * When LSTM has decided what relevant information to keep, and what to discard, it then performs some computations to store the new information, via the ___input gate___ or sometimes known as an ___external input gate___. \n",
    "    * To update the internal cell state, you'll first have to pass the previous hidden state, and the current input with the bias into a sigmoid activation function, that decides which values to update by transforming them between 0 and 1.\n",
    "\n",
    "\n",
    "\n",
    "3) ___Use the above steps to selectively update their internal state___\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4) ___Forget the irreverent history___\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
